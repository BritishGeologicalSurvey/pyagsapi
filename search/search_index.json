{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to pyagsapi - AGS File Utilities API","text":"<p>A HTTP API for the AGS Python library.</p> <p>It can:</p> <ul> <li>Validate AGS files to v4.x of the AGS data format standard</li> <li>Validate AGS files for submission to the National Geoscience Data Center (NGDC)</li> <li>Convert between AGS format files and spreadsheet files <code>.ags</code> &lt;-&gt; <code>.xlsx</code></li> <li>Download PDF logs of existing files within the National Geoscience Data Centre</li> </ul> <p>It is built on the FastAPI framework, using the official FastAPI Docker image.</p> <p>The core Python API provides the functionality to validate and convert AGS geotechnical data. From here, standard Python web frameworks like Uvicorn and Starlette provide the web API/wrapper atop the core Python API.</p> <p>BGS Deployed Instance available at: https://agsapi.bgs.ac.uk/</p>"},{"location":"#quick-start","title":"Quick start","text":""},{"location":"#docker","title":"Docker","text":"<p>The simplest way to run the validation service is via Docker:</p> <pre><code>docker run -p 80:80 --name pyagsapi ghcr.io/britishgeologicalsurvey/pyagsapi:latest\n</code></pre> <p>Navigate to http://localhost to see the landing page or http://localhost/docs to see the API documentation via the Swagger interface.</p> <p>The <code>latest</code> tag reflects the current state of the <code>main</code> branch of the repository. It may have breaking changes. Use versions from tagged Releases to fix the version in deployment pipelines. Available tags are listed in the Container Registry.</p>"},{"location":"#setting-the-root_path","title":"Setting the <code>root_path</code>","text":"<p>If you are running behind a proxy, you may need to set the <code>root_path</code> using the <code>PYAGSAPI_ROOT_PATH</code> environment variable:</p> <pre><code>docker run -p 80:80 -e PYAGSAPI_ROOT_PATH=\"/pyagsapi\" --name pyagsapi ghcr.io/britishgeologicalsurvey/pyagsapi\n</code></pre> <p>This will ensure that all references to <code>self</code> in responses, and all Swagger and REDOC documentation, include the correct path.</p>"},{"location":"#from-source","title":"From Source","text":"<p>pyagsapi runs on Python &gt;= 3.11.</p> <pre><code>python -m venv pyagsapi\nsource pyagsapi/bin/activate\ngit clone https://github.com/BritishGeologicalSurvey/pyagsapi.git\ncd pyagsapi\npip install -r requirements.txt\nuvicorn app.main:app \n</code></pre>"},{"location":"#development","title":"Development","text":"<p>The main repo for this project is https://github.com/BritishGeologicalSurvey/pyagsapi/.</p> <p>Please raise any feature requests, issues or pull requests against this repository.</p>"},{"location":"#running-locally","title":"Running locally","text":"<p>AGS Validator is written in Python and based on the FastAPI framework. It runs on the Uvicorn ASGI server.</p> <p>Use the instructions above to run the API locally.</p> <p>By default, the API is served at http://localhost:8000.</p>"},{"location":"#running-tests","title":"Running tests","text":"<p>Use the following to run the tests:</p> <pre><code>pip install -r requirements_dev.txt\nexport PYTHONPATH=.\npytest -vs test\n</code></pre> <p>The test environment is configured so that adding <code>--pdb</code> to the test command will start an IPython debugger session in the event of test failure.</p>"},{"location":"#gui-customisation","title":"GUI Customisation","text":"<p>To ammend the GUI HTML we recommend running via <code>Docker</code> using your own <code>Dockerfile</code> like the below to <code>COPY</code> in your own templates.</p> <pre><code>FROM ghcr.io/britishgeologicalsurvey/pyagsapi:2.0\n\nCOPY content/static /app/app/static\nCOPY content/templates /app/app/templates\n</code></pre>"},{"location":"#container-registry","title":"Container Registry","text":"<p>Containers for the application are hosted in the GitHub Container Registry</p> <p>Every push to <code>Main</code> branch commits builds <code>pyagsapi:latest</code>.</p> <p>Push Tagged Releases with <code>^v?[0-9]+[.][0-9]+([.][0-9])?</code> (v* == v2.0) builds <code>pyagsapi:2.0</code> (the \"v\" gets dropped for the tag).</p> <p>You can also push release candidates using the format <code>/^v?[0-9]+[.][0-9]+([.][0-9])?\\-rc/</code> e.g. v3.1.1-rc builds <code>pyagsapi:3.1.1-rc</code></p>"},{"location":"#example-files","title":"Example Files","text":"<p>Files in https://github.com/BritishGeologicalSurvey/pyagsapi/tree/main/test/files/real are a random collection of real AGS files which have been submitted to the BGS and are available under OGL, we have included them here as example files for testing pyagsapi.</p>"},{"location":"#licence","title":"Licence","text":"<p><code>pyagsapi</code> was created by and is maintained by the British Geological Survey. It is distributed under the LGPL v3.0 licence. Copyright: \u00a9 BGS / UKRI 2021</p> <p>Contains data supplied by Natural Environment Research Council.</p> <p>Contains public sector information licensed under the Open Government Licence v3.0</p>"},{"location":"bgs_rules/","title":"bgs_rules.py","text":"<p>Functions for each of the BGS data validation rules</p>"},{"location":"bgs_rules/#app.bgs_rules.check_required_groups","title":"<code>check_required_groups(tables)</code>","text":"<p>Groups must include PROJ, LOCA or HOLE, ABBR, TYPE, UNIT</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_required_groups(tables: dict) -&gt; List[dict]:\n    \"\"\" Groups must include PROJ, LOCA or HOLE, ABBR, TYPE, UNIT \"\"\"\n    errors = []\n    required = ['PROJ', 'ABBR', 'TYPE', 'UNIT']\n    missing = []\n\n    for group in required:\n        if group not in tables.keys():\n            missing.append(group)\n\n    if 'LOCA' not in tables.keys() and 'HOLE' not in tables.keys():\n        missing.append('(LOCA or HOLE)')\n\n    if missing:\n        desc = 'Required groups not present: ' + ', '.join(missing)\n        errors.append({'line': '-', 'group': '', 'desc': desc})\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_required_bgs_groups","title":"<code>check_required_bgs_groups(tables)</code>","text":"<p>Groups must include GEOL for BGS</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_required_bgs_groups(tables: dict) -&gt; List[dict]:\n    \"\"\" Groups must include GEOL for BGS \"\"\"\n    errors = []\n    required = ['GEOL']\n    missing = []\n\n    for group in required:\n        if group not in tables.keys():\n            missing.append(group)\n\n    if missing:\n        desc = 'Required BGS groups not present: ' + ', '.join(missing)\n        errors.append({'line': '-', 'group': '', 'desc': desc})\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_spatial_referencing_system","title":"<code>check_spatial_referencing_system(tables)</code>","text":"<p>Spatial referencing system defined in LOCA_GREF, LOCA_LREF or LOCA_LLZ</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_spatial_referencing_system(tables: dict) -&gt; List[dict]:\n    \"\"\" Spatial referencing system defined in LOCA_GREF, LOCA_LREF or LOCA_LLZ \"\"\"\n    ref_found = False\n    errors = []\n\n    try:\n        location = tables['LOCA']\n        for col in ['LOCA_GREF', 'LOCA_LREF', 'LOCA_LLZ']:\n            try:\n                if all(location[col] != ''):\n                    ref_found = True\n            except KeyError:\n                pass\n        if not ref_found:\n            desc = 'Spatial referencing system not in LOCA_GREF, LOCA_LREF or LOCA_LLZ!'\n            errors.append({'line': '-', 'group': 'LOCA', 'desc': desc})\n    except KeyError:\n        # LOCA not present, already checked in earlier rule\n        pass\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_eastings_northings_present","title":"<code>check_eastings_northings_present(tables)</code>","text":"<p>Eastings and Northings columns are populated</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_eastings_northings_present(tables: dict) -&gt; List[dict]:\n    \"\"\"Eastings and Northings columns are populated\"\"\"\n    errors = []\n    try:\n        location = tables['LOCA']\n        if any(location['LOCA_NATE'].isna()) or any(location['LOCA_NATE'] == 0):\n            errors.append(\n                {'line': '-', 'group': 'LOCA',\n                 'desc': 'LOCA_NATE contains zeros or null values'})\n        if any(location['LOCA_NATN'].isna()) or any(location['LOCA_NATN'] == 0):\n            errors.append(\n                {'line': '-', 'group': 'LOCA',\n                 'desc': 'LOCA_NATN contains zeros or null values'})\n    except KeyError:\n        # LOCA not present, already checked in earlier rule\n        pass\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_eastings_northings_range","title":"<code>check_eastings_northings_range(tables)</code>","text":"<p>Eastings and Northings columns fall within reasonable range</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_eastings_northings_range(tables: dict) -&gt; List[dict]:\n    \"\"\"Eastings and Northings columns fall within reasonable range\"\"\"\n    errors = []\n    try:\n        location = tables['LOCA']\n        if any(location['LOCA_NATE'] &lt; 1e5) or any(location['LOCA_NATE'] &gt; 8e5):\n            errors.append(\n                {'line': '-', 'group': 'LOCA',\n                 'desc': 'LOCA_NATE values outside 100,000 to 800,000 range'})\n        if any(location['LOCA_NATN'] &lt; 1e5) or any(location['LOCA_NATN'] &gt; 1.4e6):\n            errors.append(\n                {'line': '-', 'group': 'LOCA',\n                 'desc': 'LOCA_NATN values outside 100,000 to 1,400,000 range'})\n    except KeyError:\n        # LOCA not present, already checked in earlier rule\n        pass\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_drill_depth_present","title":"<code>check_drill_depth_present(tables)</code>","text":"<p>Drill depth value is populate and not zero</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_drill_depth_present(tables: dict) -&gt; List[dict]:\n    \"\"\"Drill depth value is populate and not zero\"\"\"\n    errors = []\n    try:\n        depth = tables['HDPH']\n        if any(depth['HDPH_TOP'].isna()):\n            errors.append(\n                {'line': '-', 'group': 'HDPH',\n                 'desc': 'HDPH_TOP contains null values'})\n        if any(depth['HDPH_BASE'].isna()) or any(depth['HDPH_BASE'] == 0):\n            errors.append(\n                {'line': '-', 'group': 'HDPH',\n                 'desc': 'HDPH_BASE contains zero or null values'})\n    except KeyError:\n        # LOCA not present, already checked in earlier rule\n        pass\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_drill_depth_geol_record","title":"<code>check_drill_depth_geol_record(tables)</code>","text":"<p>Drill depths have corresponding records in geol table</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_drill_depth_geol_record(tables: dict) -&gt; List[dict]:\n    \"\"\"Drill depths have corresponding records in geol table\"\"\"\n    errors = []\n    try:\n        depth = tables['HDPH']\n        geology = tables['GEOL']\n        geology_ids = set(geology['LOCA_ID'].unique())\n        depth_ids = set(depth['LOCA_ID'].unique())\n\n        if not_in_geology := depth_ids.difference(geology_ids):\n            errors.append(\n                {'line': '-', 'group': 'HDPH',\n                 'desc': f'HDPH LOCA_IDs not in GEOL group ({not_in_geology})'})\n        if not_in_depth := geology_ids.difference(depth_ids):\n            errors.append(\n                {'line': '-', 'group': 'HDPH',\n                 'desc': f'GEOL LOCA_IDs not in HDPH group ({not_in_depth})'})\n\n    except KeyError:\n        # LOCA not present, already checked in earlier rule\n        pass\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_loca_within_great_britain","title":"<code>check_loca_within_great_britain(tables)</code>","text":"<p>Location coordinates fall on land within Great Britain.</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_loca_within_great_britain(tables: dict) -&gt; List[dict]:\n    \"\"\"Location coordinates fall on land within Great Britain.\"\"\"\n    gb_outline = gpd.read_file(GB_OUTLINE).loc[0, 'geometry']\n    ni_outline = gpd.read_file(NI_OUTLINE).loc[0, 'geometry']\n    uk_eea_outline_wgs84 = gpd.read_file(UK_EEA_OUTLINE)\n    uk_eea_outline = uk_eea_outline_wgs84.to_crs('EPSG:27700').loc[0, 'geometry']\n    errors = []\n\n    # Read data into geodataframe\n    try:\n        location = create_location_gpd(tables)\n    except KeyError:\n        # LOCA not present, already checked in earlier rule\n        return errors\n\n    inside_uk_eea_mask = location.intersects(uk_eea_outline)\n    inside_gb_mask = location.intersects(gb_outline)\n    as_irish_grid = location.to_crs(\"EPSG:29903\")\n    inside_ni_mask = as_irish_grid.intersects(ni_outline)\n    outside_uk_eea_and_ni_mask = ~inside_uk_eea_mask &amp; ~inside_ni_mask\n    outside_gb_and_ni_mask = ~inside_gb_mask &amp; ~inside_ni_mask\n\n    for loca_id, row in location.loc[outside_uk_eea_and_ni_mask].iterrows():\n        errors.append({\n            'line': f'{row[\"line_no\"]}', 'group': 'LOCA',\n            'desc': f'NATE / NATN outside UK Offshore EEA or Onshore Northern Ireland boundary ({loca_id})'\n        })\n\n    for loca_id, row in location.loc[outside_gb_and_ni_mask].iterrows():\n        errors.append({\n            'line': f'{row[\"line_no\"]}', 'group': 'LOCA',\n            'desc': f'NATE / NATN outside Onshore Great Britain or Northern Ireland boundaries ({loca_id})'\n        })\n\n    for loca_id, row in location.loc[inside_ni_mask].iterrows():\n        if row['LOCA_GREF']:\n            continue\n        else:\n            errors.append({\n                'line': f'{row[\"line_no\"]}', 'group': 'LOCA',\n                'desc': f'NATE / NATN in Northern Ireland but LOCA_GREF undefined ({loca_id})'\n            })\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_locx_is_not_duplicate_of_other_column","title":"<code>check_locx_is_not_duplicate_of_other_column(tables)</code>","text":"<p>LOCA_LOCX and LOCA_LOCY are not duplicates of other columns</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_locx_is_not_duplicate_of_other_column(tables: dict) -&gt; List[dict]:\n    \"\"\"LOCA_LOCX and LOCA_LOCY are not duplicates of other columns\"\"\"\n\n    def check_for_duplicates(row):\n        \"\"\"Return errors for rows that contain duplicates.\"\"\"\n        error = None\n\n        if row['LOCA_NATE'] == row['LOCA_LOCX'] or row['LOCA_NATN'] == row['LOCA_LOCY']:\n            error = {'line': '-', 'group': 'LOCA',\n                     'desc': f'LOCX / LOCY duplicates NATE / NATN ({row.name})'}\n        elif row['LOCA_LON'] == '' and row['LOCA_LAT'] == '':\n            error = None\n        elif (float(row['LOCA_LON']) == row['LOCA_LOCX'] or\n              float(row['LOCA_LAT']) == row['LOCA_LOCY']):\n            error = {'line': '-', 'group': 'LOCA',\n                     'desc': f'LOCX / LOCY duplicates LON / LAT ({row.name})'}\n\n        return error\n\n    # Apply check to data\n    try:\n        location = tables['LOCA'].set_index('LOCA_ID')\n        result = location.apply(check_for_duplicates, axis=1)\n        errors = result[result.notnull()].to_list()\n    except KeyError:\n        # LOCA not present, already checked in earlier rule\n        errors = []\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_loca_id_references_are_valid","title":"<code>check_loca_id_references_are_valid(tables)</code>","text":"<p>Groups that have LOCA_ID column have populated it with valid record</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_loca_id_references_are_valid(tables: dict) -&gt; List[dict]:\n    \"\"\"Groups that have LOCA_ID column have populated it with valid record\"\"\"\n    errors = []\n    # Extract IDs from LOCA table\n    try:\n        loca_ids = tables['LOCA']['LOCA_ID'].unique()\n    except KeyError:\n        # LOCA not present, already checked in earlier rule\n        return errors\n\n    tables_referencing_loca = [table_name for table_name in tables.keys()\n                               if 'LOCA_ID' in tables[table_name].columns\n                               and table_name != 'LOCA']\n\n    # Check each table for valid references\n    for table in tables_referencing_loca:\n        # define check function here because value of `table` changes.\n        def check_loca_references(row):\n            # table, loca_ids are taken from enclosing scope\n            if not row['LOCA_ID']:\n                # Record number is 0-indexed in name column\n                error = {'line': '-', 'group': table,\n                         'desc': f'Record {row.name + 1} has missing LOCA_ID'}\n            elif row['LOCA_ID'] not in loca_ids:\n                error = {'line': '-', 'group': table,\n                         'desc': f'LOCA_ID ({row[\"LOCA_ID\"]}) is not found in LOCA group'}\n            else:\n                error = None\n\n            return error\n\n        # Run check and add result to error list\n        result = tables[table].apply(check_loca_references, axis=1)\n        errors.extend(result[result.notnull()].to_list())\n\n    return errors\n</code></pre>"},{"location":"bgs_rules/#app.bgs_rules.check_sample_referencing","title":"<code>check_sample_referencing(tables)</code>","text":"<p>If a SAMP group exists it must:  - have an identifier SAMP_ID or (LOCA_ID,SAMP_TOP,SAMP_TYPE,SAMP_REF)  - all identifiers must be unique</p> <p>For all child groups see bgs_group_id_keys:  - have an identifier matching 'samp_id' or 'comp_id' for that group  - all identifiers must be unique  - the identifier SAMP_ID or (LOCA_ID,SAMP_TOP,SAMP_TYPE,SAMP_REF)    must appear in the SAMP group</p> Source code in <code>app/bgs_rules.py</code> <pre><code>def check_sample_referencing(tables: dict) -&gt; List[dict]:\n    \"\"\"\n       If a SAMP group exists it must:\n        - have an identifier SAMP_ID or (LOCA_ID,SAMP_TOP,SAMP_TYPE,SAMP_REF)\n        - all identifiers must be unique\n\n       For all child groups see bgs_group_id_keys:\n        - have an identifier matching 'samp_id' or 'comp_id' for that group\n        - all identifiers must be unique\n        - the identifier SAMP_ID or (LOCA_ID,SAMP_TOP,SAMP_TYPE,SAMP_REF)\n          must appear in the SAMP group\n    \"\"\"\n\n    def values_all_valid(row, id_keys):\n        \"\"\" Return true if all the values are not null and not empty \"\"\"\n        for id_key in id_keys:\n            if row[id_key] is None or row[id_key] == '':\n                return False\n        return True\n\n    def id_from_keys(row, id_keys):\n        \"\"\" Concatenate values to create id \"\"\"\n        values = [str(row[id_key]) for id_key in id_keys]\n        id_ = ','.join(values)\n        return id_\n\n    def clean_ids(id_pairs: pd.DataFrame):\n        #  Remove null pairs and fill blank sample ids with composite ids\n        rows_without_any_nulls = id_pairs.notna().any(axis=1)\n        id_pairs = id_pairs.loc[rows_without_any_nulls].copy()\n        id_pairs['samp_id'].fillna(id_pairs['comp_id'], inplace=True)\n        return id_pairs\n\n    def id_pair(row, group):\n        id_keys = get_group_id_keys(group)\n\n        samp_id = None\n        if (set(id_keys['samp_id_keys']) &lt;= set(row.keys())\n                and values_all_valid(row, id_keys['samp_id_keys'])):\n            samp_id = id_from_keys(row, id_keys['samp_id_keys'])\n\n        comp_id = None\n        if (set(id_keys['comp_id_keys']) &lt;= set(row.keys())\n                and values_all_valid(row, id_keys['comp_id_keys'])):\n            comp_id = id_from_keys(row, id_keys['comp_id_keys'])\n        return pd.Series([samp_id, comp_id])\n\n    def child_consistency(samp_ids, tables: dict) -&gt; List[dict]:\n        errors = []\n        children = []\n        for group in tables.keys():\n            id_keys = get_group_id_keys(group)\n            if ((set(id_keys['samp_id_keys']) &lt;= set(tables[group].columns)\n                    or set(id_keys['comp_id_keys']) &lt;= set(tables[group].columns))\n                    and group != 'SAMP'):\n                children.append(group)\n\n        for group in children:\n            child_id_pairs = tables[group].apply(id_pair, axis=1, args=(group,))\n            child_id_pairs.columns = ['samp_id', 'comp_id']\n            errors, child_id_pairs = internal_consistency(group, child_id_pairs)\n\n            # Parent ids refer to keys used by SAMP with extra fields\n            parent_id_pairs = tables[group].apply(id_pair, axis=1, args=('SAMP',))\n            parent_id_pairs.columns = ['samp_id', 'comp_id']\n            parent_id_pairs = clean_ids(parent_id_pairs)\n\n            if no_parent_ids := sorted(list(set(parent_id_pairs['samp_id']).difference(set(samp_ids)))):\n                errors.append(\n                    {'line': '-', 'group': f'{group}',\n                     'desc': (f\"No parent id: {','.join(id_keys['samp_id_keys'])} or \"\n                              f\"({','.join(id_keys['comp_id_keys'])}) \"\n                              f\"not in SAMP group ({no_parent_ids})\")})\n        return errors\n\n    def internal_consistency(group: str, id_pairs: pd.DataFrame):\n        errors = []\n        id_keys = get_group_id_keys(group)\n\n        # Check for missing IDs\n        for row_id in id_pairs[id_pairs.isna().all(axis=1)].index.to_list():\n            errors.append(\n                {'line': '-', 'group': f'{group}',\n                 'desc': f\"Record {row_id + 1} is missing either \"\n                         f\"{','.join(id_keys['samp_id_keys'])} or \"\n                         f\"({','.join(id_keys['comp_id_keys'])})\"})\n\n        id_pairs = clean_ids(id_pairs)\n\n        # Check for duplicate IDs\n        for samp_id in sorted(list(set(id_pairs[id_pairs['samp_id'].duplicated()]['samp_id']))):\n            errors.append(\n                {'line': '-', 'group': f'{group}',\n                 'desc': (f\"Duplicate sample id {samp_id}: {','.join(id_keys['samp_id_keys'])} \"\n                          f\"or ({','.join(id_keys['comp_id_keys'])}) \"\n                          f\"must be unique\")})\n        # remove duplicate ids\n        id_pairs = id_pairs[~ id_pairs['samp_id'].duplicated()]\n\n        # Check for inconsistent IDs\n        for samp_id in id_pairs[id_pairs['comp_id'].duplicated()]['samp_id']:\n            errors.append(\n                {'line': '-', 'group': f'{group}',\n                 'desc': f'Inconsistent id {samp_id}: references duplicate component data'})\n\n        return errors, id_pairs\n\n    # Check data\n    try:\n        sample = tables['SAMP']\n        samp_id_pairs = sample.apply(id_pair, axis=1, args=('SAMP',))\n        samp_id_pairs.columns = ['samp_id', 'comp_id']\n        errors, samp_id_pairs = internal_consistency('SAMP', samp_id_pairs)\n        child_errors = child_consistency(samp_id_pairs['samp_id'], tables)\n        errors.extend(child_errors)\n    except KeyError:\n        # group not in group list\n        errors = []\n\n    return errors\n</code></pre>"},{"location":"checkers/","title":"checkers.py","text":"<p>checker functions check a file against the rules.  They also catch errors to do with opening the files.</p>"},{"location":"checkers/#app.checkers.check_bgs","title":"<code>check_bgs(filename, **kwargs)</code>","text":"<p>Validate file against BGS rules.  kwargs parameter required because some validation functions have keyword parameters.</p> Source code in <code>app/checkers.py</code> <pre><code>def check_bgs(filename: Path, **kwargs) -&gt; dict:\n    \"\"\"\n    Validate file against BGS rules.  kwargs parameter required because some\n    validation functions have keyword parameters.\n    \"\"\"\n    logger.info(\"Checking %s against BGS rules.\", filename.name)\n    errors = {}\n    load_error = None\n    bgs_metadata = {}\n\n    tables, load_error, ags4_errors = load_tables_reporting_errors(filename)\n\n    if load_error:\n        errors['File read error'] = [{'line': '-', 'group': '', 'desc': load_error}]\n    else:\n        errors.update(ags4_errors)\n        # Get additional metadata\n        bgs_metadata = generate_bgs_metadata(tables)\n\n        # Apply checks\n        for rule, func in BGS_RULES.items():\n            logger.info(\"Checking against %s\", rule)\n            result = func(tables)\n            if result:\n                errors[rule] = result\n\n    return dict(checker=f'bgs_rules v{bgs_rules_version}',\n                errors=errors,\n                additional_metadata=bgs_metadata)\n</code></pre>"},{"location":"checkers/#app.checkers.generate_bgs_metadata","title":"<code>generate_bgs_metadata(tables)</code>","text":"<p>Generate additional metadata from groups.</p> Source code in <code>app/checkers.py</code> <pre><code>def generate_bgs_metadata(tables: Dict[str, pd.DataFrame]) -&gt; dict:\n    \"\"\"Generate additional metadata from groups.\"\"\"\n    try:\n        projects = tables['PROJ'].apply(lambda row: f\"{row['PROJ_ID']} ({row['PROJ_NAME']})\", axis=1).to_list()\n    except KeyError:\n        projects = []\n\n    try:\n        loca_rows = len(tables['LOCA'][tables['LOCA']['HEADING'] == 'DATA'])\n    except KeyError:\n        loca_rows = 0\n\n    groups = tables.keys()\n    bgs_metadata = {\n        'bgs_all_groups': f'{len(groups)} groups identified in file: {\" \".join(groups)}',\n        'bgs_file': f'Optional FILE group present: {\"FILE\" in groups}',\n        'bgs_dict': f'Optional DICT group present: {\"DICT\" in groups}',\n        'bgs_loca_rows': f'{loca_rows} data row(s) in LOCA group',\n        'bgs_projects': f'{len(projects)} projects found: {\"; \".join(projects)}',\n    }\n    return bgs_metadata\n</code></pre>"},{"location":"checkers/#app.checkers.load_ags4_as_numeric","title":"<code>load_ags4_as_numeric(filename)</code>","text":"<p>Read AGS4 file and convert to numeric data types.</p> Source code in <code>app/checkers.py</code> <pre><code>def load_ags4_as_numeric(filename: Path) -&gt; Tuple[dict, dict, List[dict]]:\n    \"\"\"Read AGS4 file and convert to numeric data types.\"\"\"\n    tables, headings = AGS4.AGS4_to_dataframe(filename)\n\n    # Check the TYPE of coordinate in LOCA\n    coord_columns = ['LOCA_NATE', 'LOCA_NATN', 'LOCA_LOCX', 'LOCA_LOCY']\n    errors = get_coord_column_type_errors(tables, coord_columns)\n\n    # Convert tables to numeric data for analysis\n    for group, df in tables.items():\n        tables[group] = AGS4.convert_to_numeric(df)\n\n    # Force conversion of LOCA coordinate columns, even if type is not numeric\n    if tables:\n        for column in coord_columns:\n            try:\n                tables['LOCA'][column] = pd.to_numeric(tables['LOCA'][column])\n            except KeyError:\n                # Not all files have all columns\n                pass\n\n    return tables, headings, errors\n</code></pre>"},{"location":"checkers/#app.checkers.get_coord_column_type_errors","title":"<code>get_coord_column_type_errors(tables, coord_columns)</code>","text":"<p>Check the coordinate columns in LOCA table have correct data type and return errors for those that don't.</p> Source code in <code>app/checkers.py</code> <pre><code>def get_coord_column_type_errors(tables: dict, coord_columns: List[str]) -&gt; dict:\n    \"\"\"\n    Check the coordinate columns in LOCA table have correct data type and\n    return errors for those that don't.\n    \"\"\"\n    try:\n        loca = tables['LOCA']\n    except KeyError:\n        # If LOCA doesn't exist, other errors are returned elsewhere\n        return {}\n\n    bad_columns = []\n    for column in coord_columns:\n        try:\n            type_ = loca.loc[loca['HEADING'] == 'TYPE', column].tolist()[0]\n            if not re.search(r'(DP|MC|SF|SCI)', type_):\n                bad_columns.append(f\"{column} ({type_})\")\n        except KeyError:\n            # Ignore columns that don't exist\n            pass\n\n    if bad_columns:\n        error_message = f\"Coordinate columns have non-numeric TYPE: {', '.join(bad_columns)}\"\n        errors = {\"BGS data validation: Non-numeric coordinate types\":\n                  [{'line': '-', 'group': 'LOCA', 'desc': error_message}]}\n    else:\n        errors = {}\n\n    return errors\n</code></pre>"},{"location":"conversion/","title":"conversion.py","text":""},{"location":"conversion/#app.conversion.convert","title":"<code>convert(filename, results_dir, sorting_strategy=None)</code>","text":"<p>Convert filename between .ags and .xlsx.  Write output to file in results_dir and return path alongside job status data in dictionary.</p> Source code in <code>app/conversion.py</code> <pre><code>def convert(filename: Path, results_dir: Path, sorting_strategy: Optional[str] = None) -&gt; Tuple[Optional[Path], dict]:\n    \"\"\"\n    Convert filename between .ags and .xlsx.  Write output to file in\n    results_dir and return path alongside job status data in dictionary.\"\"\"\n    # Prepare variables and directory\n    new_extension = '.ags' if filename.suffix == '.xlsx' else '.xlsx'\n    converted_file = results_dir / (filename.stem + new_extension)\n    logger.info(\"Converting %s to %s\", filename.name, converted_file.name)\n    if not results_dir.exists():\n        results_dir.mkdir()\n\n    # Prepare response with metadata\n    response = _prepare_response_metadata(filename)\n\n    # Do the conversion\n    success = False\n    if filename.suffix.lower() == '.ags':\n        try:\n            AGS4.AGS4_to_excel(filename, converted_file, sorting_strategy=sorting_strategy)\n            success = True\n        except IndexError:\n            error_message = \"ERROR: File does not have AGS4 format layout\"\n        except UnboundLocalError:\n            # This error is thrown in response to a bug in the upstream code,\n            # which in turn is only triggered if the AGS file has duplicate\n            # headers.\n            error_message = \"ERROR: File contains duplicate headers\"\n        except AGS4.AGS4Error as err:\n            error_message = str(err)\n    elif filename.suffix == '.xlsx':\n        try:\n            AGS4.excel_to_AGS4(filename, converted_file)\n            if converted_file.exists():\n                success = True\n            else:\n                # The underlying conversion fails silently,\n                # the file does not then exist.\n                # Propagate a short error message\n                error_message = \"ERROR: Conversion failed\"\n        except AttributeError as err:\n            # Include error details here in case they provide a clue e.g. which\n            # attribute is missing\n            error_message = f\"ERROR: Bad spreadsheet layout ({err.args[0]})\"\n    else:\n        error_message = f\"ERROR: {filename.name} is not .ags or .xlsx format\"\n\n    # Update response and clean failed files\n    if success:\n        response['message'] = f\"SUCCESS: {filename.name} converted to {converted_file.name}\"\n        response['valid'] = True\n    else:\n        response['message'] = error_message\n        response['valid'] = False\n        converted_file.unlink(missing_ok=True)\n        converted_file = None\n\n    return (converted_file, response)\n</code></pre>"},{"location":"errors/","title":"errors.py","text":""},{"location":"main/","title":"main.py","text":""},{"location":"main/#app.main.setup_logging","title":"<code>setup_logging(logging_level=logging.INFO)</code>","text":"<p>Explicitly configure all loggers</p> Source code in <code>app/main.py</code> <pre><code>def setup_logging(logging_level=logging.INFO):\n    \"\"\"Explicitly configure all loggers\"\"\"\n\n    # Create console handler\n    ch = logging.StreamHandler()\n    console_formatter = colorlog.ColoredFormatter(\n        '%(log_color)s%(levelname)s%(reset)s | %(asctime)s | %(name)s | %(message)s')\n    ch.setFormatter(console_formatter)\n    ch.setLevel(logging_level)\n\n    # Configure request logger\n    request_logger = logging.getLogger('request')\n    request_logger.setLevel(logging_level)\n    request_logger.handlers.clear()\n    request_logger.addHandler(ch)\n    request_logger.propagate = False\n\n    # Configure uvicorn loggers\n    logging.getLogger('uvicorn.access').handlers.clear()\n    logging.getLogger('uvicorn.access').addHandler(ch)\n    # INFO logs all requests including regular internal checks\n    # which occur every few seconds. Turn down to WARNING\n    logging.getLogger('uvicorn.access').setLevel(logging.WARNING)\n    logging.getLogger('uvicorn.access').propagate = False\n\n    logging.getLogger('uvicorn.error').handlers.clear()\n    logging.getLogger('uvicorn.error').addHandler(ch)\n    logging.getLogger('uvicorn.error').propagate = False\n\n    # Configure app logger\n    # Log application startup (these messages appear once for each uvicorn\n    # worker as it starts).\n    app_logger = logging.getLogger('app')\n    app_logger.setLevel(logging_level)\n    app_logger.addHandler(ch)\n    app_logger.propagate = True\n\n    # Start logging\n    app_logger.info(\n        f\"Starting app instance: \"\n        f\"'logging_level': {logging.getLevelName(logging_level)}\")\n</code></pre>"},{"location":"routes/","title":"routes.py","text":""},{"location":"routes/#app.routes.validate","title":"<code>validate(background_tasks, files=validation_file, std_dictionary=dictionary_form, checkers=validate_form, fmt=format_form, return_geometry=geometry_form, request=None)</code>  <code>async</code>","text":"<p>Validate an AGS4 file to the AGS File Format v4.x rules and the NGDC data submission requirements. Uses the Official AGS4 Python Library. :param background_tasks: Background tasks for deleting temporary directories. :type background_tasks: BackgroundTasks :param files: List of AGS4 files to be validated. :type files: List[UploadFile] :param std_dictionary: The standard dictionary to use for validation. Options are \"BGS\" or \"AGS\". :type std_dictionary: Dictionary :param checkers: List of validation rules to be used during validation. :type checkers: List[Checker]</p> <p>:param fmt: The format to return the validation results in. Options are \"text\" or \"json\". :type fmt: Format :param return_geometry: Include GeoJSON in validation response. Options are True or False. :type return_geometry: bool :param request: The request object. :type request: Request :return: A response with the validation results in either plain text or JSON format. :rtype: Union[FileResponse, ValidationResponse] :raises InvalidPayloadError: If the payload is missing files or checkers.</p> Source code in <code>app/routes.py</code> <pre><code>@router.post(\"/validate/\",\n             tags=[\"validate\"],\n             response_model=ValidationResponse,\n             responses=log_responses,\n             summary=\"Validate AGS4 File(s)\",\n             description=(\"Validate an AGS4 file to the AGS File Format v4.x rules and the NGDC data\"\n                          \" submission requirements. Uses the Offical AGS4 Python Library.\"))\nasync def validate(background_tasks: BackgroundTasks,\n                   files: List[UploadFile] = validation_file,\n                   std_dictionary: Dictionary = dictionary_form,\n                   checkers: List[Checker] = validate_form,\n                   fmt: Format = format_form,\n                   return_geometry: bool = geometry_form,\n                   request: Request = None):\n    \"\"\"\n    Validate an AGS4 file to the AGS File Format v4.x rules and the NGDC data submission requirements.\n    Uses the Official AGS4 Python Library.\n    :param background_tasks: Background tasks for deleting temporary directories.\n    :type background_tasks: BackgroundTasks\n    :param files: List of AGS4 files to be validated.\n    :type files: List[UploadFile]\n    :param std_dictionary: The standard dictionary to use for validation. Options are \"BGS\" or \"AGS\".\n    :type std_dictionary: Dictionary\n    :param checkers: List of validation rules to be used during validation.\n    :type checkers: List[Checker]\n\n    :param fmt: The format to return the validation results in. Options are \"text\" or \"json\".\n    :type fmt: Format\n    :param return_geometry: Include GeoJSON in validation response. Options are True or False.\n    :type return_geometry: bool\n    :param request: The request object.\n    :type request: Request\n    :return: A response with the validation results in either plain text or JSON format.\n    :rtype: Union[FileResponse, ValidationResponse]\n    :raises InvalidPayloadError: If the payload is missing files or checkers.\n    \"\"\"\n\n    if not files[0].filename or not checkers:\n        raise InvalidPayloadError(request)\n\n    checkers = [checker_functions[c] for c in checkers]\n\n    tmp_dir = Path(tempfile.mkdtemp())\n    background_tasks.add_task(shutil.rmtree, tmp_dir)\n\n    if std_dictionary == Dictionary.None_Given:\n        dictionary = None\n    else:\n        dictionary = f'Standard_dictionary_{std_dictionary}.ags'\n\n    data = []\n    for file in files:\n        contents = await file.read()\n        local_ags_file = tmp_dir / file.filename\n        local_ags_file.write_bytes(contents)\n        result = validation.validate(\n            local_ags_file, checkers=checkers, standard_AGS4_dictionary=dictionary)\n        if return_geometry:\n            try:\n                geojson = extract_geojson(local_ags_file)\n                result['geojson'] = geojson\n            except ValueError as ve:\n                result['geojson'] = {}\n                result['geojson_error'] = str(ve)\n        data.append(result)\n\n    if fmt == Format.TEXT:\n        full_logfile = tmp_dir / 'results.log'\n        with full_logfile.open('wt') as f:\n            f.write('=' * 80 + '\\n')\n            for result in data:\n                log = validation.to_plain_text(result)\n                f.write(log)\n                f.write('=' * 80 + '\\n')\n        response = FileResponse(full_logfile, media_type=\"text/plain\")\n    else:\n        response = prepare_validation_response(request, data)\n\n    return response\n</code></pre>"},{"location":"routes/#app.routes.prepare_validation_response","title":"<code>prepare_validation_response(request, data)</code>","text":"<p>Package the data into a Response schema object</p> Source code in <code>app/routes.py</code> <pre><code>def prepare_validation_response(request, data):\n    \"\"\"Package the data into a Response schema object\"\"\"\n    response_data = {\n        'msg': f'{len(data)} files validated',\n        'type': 'success',\n        'self': get_request_url(request),\n        'data': data,\n    }\n    return ValidationResponse(**response_data, media_type=\"application/json\")\n</code></pre>"},{"location":"routes/#app.routes.convert","title":"<code>convert(background_tasks, files=conversion_file, sort_tables=sort_tables_form, request=None)</code>  <code>async</code>","text":"<p>Convert files between .ags and .xlsx format. Option to sort worksheets in .xlsx file in alphabetical order. :param background_tasks: A background task that manages file conversion asynchronously. :type background_tasks: BackgroundTasks :param files: A list of files to be converted. Must be in .ags or .xlsx format. :type files: List[UploadFile] :param sort_tables: A boolean indicating whether to sort worksheets in the .xlsx file in alphabetical order. :type sort_tables: bool :param request: The HTTP request object. :type request: Request :return: A streaming response containing a .zip file with the converted files and a log file. :rtype: StreamingResponse :raises InvalidPayloadError: If the request payload is invalid. :raises Exception: If the conversion fails or an unexpected error occurs.</p> Source code in <code>app/routes.py</code> <pre><code>@router.post(\"/convert/\",\n             tags=[\"convert\"],\n             response_class=StreamingResponse,\n             responses=zip_responses,\n             summary=\"Convert files between .ags and .xlsx format\",\n             description=(\"Convert files between .ags and .xlsx format. Option to\"\n                          \" sort worksheets in .xlsx file in alphabetical order.\"))\nasync def convert(background_tasks: BackgroundTasks,\n                  files: List[UploadFile] = conversion_file,\n                  sort_tables: str = sort_tables_form,\n                  request: Request = None):\n    \"\"\"\n    Convert files between .ags and .xlsx format. Option to sort worksheets in .xlsx file in alphabetical order.\n    :param background_tasks: A background task that manages file conversion asynchronously.\n    :type background_tasks: BackgroundTasks\n    :param files: A list of files to be converted. Must be in .ags or .xlsx format.\n    :type files: List[UploadFile]\n    :param sort_tables: A boolean indicating whether to sort worksheets in the .xlsx file in alphabetical order.\n    :type sort_tables: bool\n    :param request: The HTTP request object.\n    :type request: Request\n    :return: A streaming response containing a .zip file with the converted files and a log file.\n    :rtype: StreamingResponse\n    :raises InvalidPayloadError: If the request payload is invalid.\n    :raises Exception: If the conversion fails or an unexpected error occurs.\n    \"\"\"\n\n    if sort_tables == SortingStrategy.default:\n        sort_tables = None\n    if not files[0].filename:\n        raise InvalidPayloadError(request)\n    RESULTS = 'results'\n    tmp_dir = Path(tempfile.mkdtemp())\n    results_dir = tmp_dir / RESULTS\n    results_dir.mkdir()\n    full_logfile = results_dir / 'conversion.log'\n    with full_logfile.open('wt') as f:\n        f.write('=' * 80 + '\\n')\n        for file in files:\n            contents = await file.read()\n            local_file = tmp_dir / file.filename\n            local_file.write_bytes(contents)\n            _, result = conversion.convert(local_file, results_dir, sorting_strategy=sort_tables)\n            log = validation.to_plain_text(result)\n            f.write(log)\n            f.write('\\n' + '=' * 80 + '\\n')\n    zipped_file = tmp_dir / RESULTS\n    shutil.make_archive(zipped_file, 'zip', results_dir)\n    zipped_stream = open(tmp_dir / (RESULTS + '.zip'), 'rb')\n\n    background_tasks.add_task(zipped_stream.close)\n    background_tasks.add_task(shutil.rmtree, tmp_dir)\n\n    response = StreamingResponse(zipped_stream, media_type=\"application/x-zip-compressed\")\n    response.headers[\"Content-Disposition\"] = f\"attachment; filename={RESULTS}.zip\"\n    return response\n</code></pre>"},{"location":"routes/#app.routes.get_ags_log","title":"<code>get_ags_log(bgs_loca_id=ags_log_query, response_type=response_type_query)</code>","text":"<p>Get a graphical log (.pdf) for a single borehole in AGS format from the National Geoscience Data Centre. :param bgs_loca_id: The unique identifier of the borehole to generate the log for. :type bgs_loca_id: str :param response_type: The type of response to return (e.g. 'attachment' to force download or 'inline'     to display in browser). :type response_type: ResponseType, optional :return: A response containing a .pdf file with the generated borehole log. :rtype: Response :raises HTTPException 404: If the specified borehole does not exist or is confidential. :raises HTTPException 500: If the borehole generator returns an error. :raises HTTPException 500: If the borehole generator could not be reached.</p> Source code in <code>app/routes.py</code> <pre><code>@router.get(\"/ags_log/\",\n            tags=[\"ags_log\"],\n            summary=\"Generate Graphical Log\",\n            description=(\"Generate a graphical log (.pdf) from AGS data \"\n                         \"held by the National Geoscience Data Centre.\"),\n            response_class=Response,\n            responses=pdf_responses)\ndef get_ags_log(bgs_loca_id: str = ags_log_query,\n                response_type: ResponseType = response_type_query):\n    \"\"\"\n    Get a graphical log (.pdf) for a single borehole in AGS format from the National Geoscience Data Centre.\n    :param bgs_loca_id: The unique identifier of the borehole to generate the log for.\n    :type bgs_loca_id: str\n    :param response_type: The type of response to return (e.g. 'attachment' to force download or 'inline' \\\n    to display in browser).\n    :type response_type: ResponseType, optional\n    :return: A response containing a .pdf file with the generated borehole log.\n    :rtype: Response\n    :raises HTTPException 404: If the specified borehole does not exist or is confidential.\n    :raises HTTPException 500: If the borehole generator returns an error.\n    :raises HTTPException 500: If the borehole generator could not be reached.\n    \"\"\"\n\n    url = BOREHOLE_VIEWER_URL.format(bgs_loca_id=bgs_loca_id)\n\n    try:\n        response = requests.get(url, timeout=10)\n    except (Timeout, ConnectionError):\n        raise HTTPException(status_code=500,\n                            detail=\"The borehole generator could not be reached.  Please try again later.\")\n\n    try:\n        response.raise_for_status()\n    except HTTPError:\n        if response.status_code == 404:\n            raise HTTPException(status_code=404,\n                                detail=f\"Failed to retrieve borehole {bgs_loca_id}. \"\n                                \"It may not exist or may be confidential\")\n        else:\n            raise HTTPException(status_code=500,\n                                detail=\"The borehole generator returned an error.\")\n\n    filename = f\"{bgs_loca_id}_log.pdf\"\n    headers = {'Content-Disposition': f'{response_type.value}; filename=\"{filename}\"'}\n\n    return Response(response.content, headers=headers, media_type='application/pdf')\n</code></pre>"},{"location":"routes/#app.routes.ags_export","title":"<code>ags_export(bgs_loca_id=ags_export_query)</code>","text":"<p>Export a single borehole in .ags format from AGS data held by the National Geoscience Data Centre. :param bgs_loca_id: The unique identifier of the borehole to export. :type bgs_loca_id: str :return: A response containing a .zip file with the exported borehole data. :rtype: Response :raises HTTPException 404: If the specified boreholes do not exist or are confidential. :raises HTTPException 422: If more than BOREHOLE_EXPORT_LIMIT borehole IDs are supplied. :raises HTTPException 500: If the borehole exporter returns an error. :raises HTTPException 500: If the borehole exporter could not be reached.</p> Source code in <code>app/routes.py</code> <pre><code>@router.get(\"/ags_export/\",\n            tags=[\"ags_export\"],\n            summary=\"Export one or more boreholes in .ags format\",\n            description=(\"Export one or more borehole in .ags format from AGS data \"\n                         \"held by the National Geoscience Data Centre.\"),\n            response_class=Response,\n            responses=ags_export_responses)\ndef ags_export(bgs_loca_id: str = ags_export_query):\n    \"\"\"\n    Export a single borehole in .ags format from AGS data held by the National Geoscience Data Centre.\n    :param bgs_loca_id: The unique identifier of the borehole to export.\n    :type bgs_loca_id: str\n    :return: A response containing a .zip file with the exported borehole data.\n    :rtype: Response\n    :raises HTTPException 404: If the specified boreholes do not exist or are confidential.\n    :raises HTTPException 422: If more than BOREHOLE_EXPORT_LIMIT borehole IDs are supplied.\n    :raises HTTPException 500: If the borehole exporter returns an error.\n    :raises HTTPException 500: If the borehole exporter could not be reached.\n    \"\"\"\n\n    if len(bgs_loca_id.split(';')) &gt; BOREHOLE_EXPORT_LIMIT:\n        raise HTTPException(status_code=422, detail=f\"More than {BOREHOLE_EXPORT_LIMIT} borehole IDs.\")\n\n    url = BOREHOLE_EXPORT_URL.format(bgs_loca_id=bgs_loca_id)\n\n    try:\n        response = requests.get(url, timeout=10)\n    except (Timeout, ConnectionError):\n        raise HTTPException(status_code=500,\n                            detail=\"The borehole exporter could not be reached.  Please try again later.\")\n\n    try:\n        response.raise_for_status()\n    except HTTPError:\n        if response.status_code == 404:\n            raise HTTPException(status_code=404,\n                                detail=f\"Failed to retrieve borehole {bgs_loca_id}. \"\n                                \"It may not exist or may be confidential\")\n        else:\n            raise HTTPException(status_code=500,\n                                detail=\"The borehole exporter returned an error.\")\n\n    headers = {'Content-Disposition': 'attachment; filename=\"boreholes.zip\"'}\n\n    return Response(response.content, headers=headers, media_type='application/x-zip-compressed')\n</code></pre>"},{"location":"routes/#app.routes.ags_export_by_polygon","title":"<code>ags_export_by_polygon(polygon=polygon_query, count_only=count_only_query, request=None)</code>","text":"<p>Export the boreholes in .ags format from AGS data held by the National Geoscience Data Centre, that are bounded by the polygon. If there are more than 50 boreholes return an error :param polygon: A polygon in Well Known Text. :type polygon: str :param count_only: The format to return the validation results in. Options are \"text\" or \"json\". :type count_only: int :param request: The request object. :type request: Request :return: A response with the validation results in either plain text or JSON format. :rtype: Union[BoreholeCountResponse, Response] :return: A response containing a count or a .zip file with the exported borehole data. :rtype: Response :raises HTTPException 422: If there are no boreholes or more than BOREHOLE_EXPORT_LIMIT boreholes in the polygon. :raises HTTPException 422: If the Well Known Text is not a POLYGON or is invalid. :raises HTTPException 500: If the borehole index could not be reached. :raises HTTPException 500: If the borehole index returns an error. :raises HTTPException 500: If the borehole exporter could not be reached. :raises HTTPException 500: If the borehole exporter returns an error.</p> Source code in <code>app/routes.py</code> <pre><code>@router.get(\"/ags_export_by_polygon/\",\n            tags=[\"ags_export_by_polygon\"],\n            summary=\"Export a number of boreholes in .ags format in a polygon\",\n            description=(\"Export a number of boreholes in .ags format from AGS data \"\n                         \"held by the National Geoscience Data Centre, using a\"\n                         \" polygon using Well-Known-Text.\"),\n            response_model=BoreholeCountResponse,\n            responses=ags_export_responses)\ndef ags_export_by_polygon(polygon: str = polygon_query,\n                          count_only: bool = count_only_query,\n                          request: Request = None):\n    \"\"\"\n    Export the boreholes in .ags format from AGS data held by the National Geoscience Data Centre,\n    that are bounded by the polygon. If there are more than 50 boreholes return an error\n    :param polygon: A polygon in Well Known Text.\n    :type polygon: str\n    :param count_only: The format to return the validation results in. Options are \"text\" or \"json\".\n    :type count_only: int\n    :param request: The request object.\n    :type request: Request\n    :return: A response with the validation results in either plain text or JSON format.\n    :rtype: Union[BoreholeCountResponse, Response]\n    :return: A response containing a count or a .zip file with the exported borehole data.\n    :rtype: Response\n    :raises HTTPException 422: If there are no boreholes or more than BOREHOLE_EXPORT_LIMIT boreholes in the polygon.\n    :raises HTTPException 422: If the Well Known Text is not a POLYGON or is invalid.\n    :raises HTTPException 500: If the borehole index could not be reached.\n    :raises HTTPException 500: If the borehole index returns an error.\n    :raises HTTPException 500: If the borehole exporter could not be reached.\n    :raises HTTPException 500: If the borehole exporter returns an error.\n    \"\"\"\n\n    # Check explicitly that the WKT is a valid POLYGON\n    # The BOREHOLE_INDEX_URL API does not return an error for some bad WKT\n    try:\n        shapely.wkt.loads(polygon)\n    except shapely.errors.GEOSException:\n        raise HTTPException(status_code=422,\n                            detail=\"Invalid polygon\")\n\n    url = BOREHOLE_INDEX_URL.format(polygon=polygon)\n\n    try:\n        response = requests.get(url, timeout=10)\n    except (Timeout, ConnectionError):\n        raise HTTPException(status_code=500,\n                            detail=\"The borehole index could not be reached.  Please try again later.\")\n\n    try:\n        response.raise_for_status()\n    except HTTPError:\n        if response.status_code == 404:\n            raise HTTPException(status_code=404,\n                                detail=\"Failed to retrieve boreholes for the given polygon\")\n        else:\n            raise HTTPException(status_code=500,\n                                detail=\"The borehole index returned an error.\")\n\n    collection = response.json()\n    count = collection['numberMatched']\n\n    if count_only:\n        response = prepare_count_response(request, count)\n    else:\n        if count == 0:\n            raise HTTPException(status_code=422,\n                                detail=\"No boreholes found in the given polygon\")\n        elif count &gt; BOREHOLE_EXPORT_LIMIT:\n            raise HTTPException(status_code=422,\n                                detail=f\"More than {BOREHOLE_EXPORT_LIMIT} boreholes ({count}) \"\n                                \"found in the given polygon. Please try with a smaller polygon\")\n\n        bgs_loca_ids = ';'.join([f['id'] for f in collection['features']])\n        url = BOREHOLE_EXPORT_URL.format(bgs_loca_id=bgs_loca_ids)\n        response = ags_export(bgs_loca_ids)\n\n    return response\n</code></pre>"},{"location":"routes/#app.routes.prepare_count_response","title":"<code>prepare_count_response(request, count)</code>","text":"<p>Package the data into a BoreholeCountResponse schema object</p> Source code in <code>app/routes.py</code> <pre><code>def prepare_count_response(request, count):\n    \"\"\"Package the data into a BoreholeCountResponse schema object\"\"\"\n    response_data = {\n        'msg': 'Borehole count',\n        'type': 'success',\n        'self': get_request_url(request),\n        'count': count\n    }\n    return BoreholeCountResponse(**response_data, media_type=\"application/json\")\n</code></pre>"},{"location":"routes/#app.routes.get_request_url","title":"<code>get_request_url(request)</code>","text":"<p>External calls need https to be returned, so check environment.</p> Source code in <code>app/routes.py</code> <pre><code>def get_request_url(request):\n    \"\"\" External calls need https to be returned, so check environment.\"\"\"\n    request_url = str(request.url)\n    if AGS_API_ENV == 'PRODUCTION' and request_url.startswith('http:'):\n        request_url = request_url.replace('http:', 'https:')\n\n    return request_url\n</code></pre>"},{"location":"validation/","title":"validation.py","text":"<p>The validate function calls checkers to validate files and combines the results in the requested format.</p>"},{"location":"validation/#app.validation.validate","title":"<code>validate(filename, checkers=[check_ags], standard_AGS4_dictionary=None)</code>","text":"<p>Validate filename (against optional dictionary) and respond in dictionary suitable for converting to JSON.</p> <p>:raises ValueError: Raised if dictionary provided is not available.</p> Source code in <code>app/validation.py</code> <pre><code>def validate(filename: Path,\n             checkers: List[Callable[[Path], dict]] = [check_ags],\n             standard_AGS4_dictionary: Optional[str] = None) -&gt; dict:\n    \"\"\"\n    Validate filename (against optional dictionary) and respond in\n    dictionary suitable for converting to JSON.\n\n    :raises ValueError: Raised if dictionary provided is not available.\n    \"\"\"\n    logger.info(\"Validate called for %s\", filename.name)\n\n    # Prepare response with metadata\n    response = _prepare_response_metadata(filename)\n\n    # Select dictionary file if exists\n    if standard_AGS4_dictionary:\n        try:\n            dictionary_file = STANDARD_DICTIONARIES[standard_AGS4_dictionary]\n        except KeyError:\n            msg = (f\"{standard_AGS4_dictionary} not available.  \"\n                   f\"Installed dictionaries: {STANDARD_DICTIONARIES.keys()}\")\n            raise ValueError(msg)\n    else:\n        dictionary_file = None\n\n    all_errors = {}\n    all_checkers = []\n    additional_metadata_responses = {'bgs': {}, 'ags': {}}\n    # Don't process if file is not .ags format\n    if filename.suffix.lower() != '.ags':\n        all_errors.update(\n            {'File read error': [\n                {'line': '-', 'group': '', 'desc': f'{filename.name} is not an .ags file'}\n            ]})\n    else:\n        # Run checkers to extract errors and other metadata\n        for checker in checkers:\n            # result is a dictionary with 'errors', 'checker' and other keys\n            result: dict = checker(filename, standard_AGS4_dictionary=dictionary_file)\n\n            # Extract 'common' data\n            all_errors.update(result.pop('errors'))\n            current_checker = result.pop('checker')\n            all_checkers.append(current_checker)\n\n            additional_metadata = result.pop('additional_metadata')\n            if current_checker.startswith('bgs_rules'):\n                additional_metadata_responses['bgs'] = additional_metadata\n            else:\n                additional_metadata_responses['ags'] = additional_metadata\n\n            # Add remaining keys to response\n            response.update(result)\n\n    # Use BGS metadata where available, as it contains more data\n    if additional_metadata_responses['bgs']:\n        response['additional_metadata'] = additional_metadata_responses['bgs']\n    else:\n        response['additional_metadata'] = additional_metadata_responses['ags']\n\n    error_count = len(reduce(lambda total, current: total + current, all_errors.values(), []))\n    if error_count &gt; 0:\n        message = f'{error_count} error(s) found in file!'\n        valid = False\n    else:\n        message = 'All checks passed!'\n        valid = True\n\n    response.update(errors=all_errors, message=message, valid=valid, checkers=all_checkers)\n\n    return response\n</code></pre>"},{"location":"validation/#app.validation.to_plain_text","title":"<code>to_plain_text(response)</code>","text":"<p>Take JSON response from convert and render as plain text.</p> Source code in <code>app/validation.py</code> <pre><code>def to_plain_text(response: dict) -&gt; str:\n    \"\"\"Take JSON response from convert and render as plain text.\"\"\"\n    return PLAIN_TEXT_TEMPLATE.render(response)\n</code></pre>"}]}